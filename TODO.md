
TODO
====


# Актуальная инфа


## Performance profiling


### Hamming Distance between two bitwise arrays

- 28.4 s - XOR
- 12.6 s - bit_count
- 10.3 s - results to list
- 1.145 s - sum results

### Other operations

- 0.8 s - get_correlation()
- 0.95 s - get_correlation_with_offset()
- 0.6 s - get_cross_correlation
- 0.03 s - other



# jsonl.py

- loads
- Создать генератор, возвращающий куски размера chunk_size из файла.
  - Открываем файл, читаем по chunk_size строк, конвертим их в список, и yield.


# profiler.py

- добавить проверку корреляций - генерим с определённым сидом, а результаты проверяем по сохранённому коду первого образца


# fingerprint.py:

- Профайлер сообщает: при поиске совпадений время расходуется на:
  - 71% - <listcomp> [(a ^ b).bit_count() for a, b in zip(nums_a, nums_b)]
  - ещё 22% - конкретно на bit_count()
  - 2% - на суммирование
  - и 1% - на остальные операции в get_correlation
  - всего 95% на вычисление дистанции Хаммонда между двумя 2D масcивами бит
  - сократив задачу в 50 раз, мы получим 7% времени, или ускорение в 15 раз.
  - Ещё по 2 % составляют движения по массиву - оффсеты
    - в идеален - найти им замену в виде функции кросс-корреляции 2D-битового массива. Если удастся делать это в 40 раз быстрее - здорово! Если можно положить на видеокарту, в 2000 раз быстрее - здорово! Ускорение составит 1000 раз. Вместо суток - 1,5 минуты. 3 минуты на 1 проход фонотеки, 15 часов на полноценный кросс.
- изучить ключи запуска fpcalc, установить лучшие, удалить sample_time - пусть
  сканируется весь файл (записать это в README)
- запуск fpcalc наиболее производительным способом, получение из него bit(byte)array
- профили пользователей - в profiles.ini
- вынести threshold в настройки, устанавливать 0.58 по умолчанию (basic), а aggressive - искать такой, ниже которого не было фактических пар в моей истории
  - собирать историю одобренных человеком пар
    - интерфейс для человека с прокруткой аудио, кнопкой "Не похож на эталон" (эталоном всегда является первый трек), кнопкой "повтор", которая объединяет мету с метой эталона, и отправляет в корзину, а если нет её - то в указанную для дублей папку
- если вместо музыкальной папки - сразу 2 Enter, то никаких папок с музыкой не создаётся, и недоступно сканирование фингерпринтов.
- установки по умолчанию перенести в setup.py, он будет создавать файл настроек main.ini, и копировать туда в секцию fingerprint
- ознакомиться с тем, как вообще работает расчёт корреляции, на что влияет  sample_time (удалить его?), min_overlap, span, step
- replace legacy calls to 3.11 recommendations (https://github.com/kdave/audio-compare)

- проверить предположения по уменьшению потребления ресурсов (времени и памяти):
  - перед сканированием фингерпринтов перемешать равномерно flac и другие файлы
    - shuffle, и проверить равномерность чем нибудь
    - если мало - то просто сортировать по расширениям, и чередовать по пропорциям
	  - при этом в каждом наборе нужно идти по порядку записи (по порядку папок?)
        чтобы HDD меньше переносил голову, и быстрее читал
  - обработка готовых кусков с результатами:
    - сохранение только тех пар, которые превышают threshold
    - запуск отдельного процесса для сохранения куска результатов, завершение контролируем только перед завершением поиска корреляций
  - проверка, есть ли сохранённые корреляции:
    - сразу при новой итерации, перед созданием чанка, создаём имя чанка. И проверяем, есть ли сохранённый результат с этим чанком? если есть - то переходим к следующей итерации - однако если чанки различны по длине - то как это сделать?
  - проверить предположения:
    - другой алгоритм смещения:
    - сохранение и загрузка отпечатка как bits-2D-array, обработка всего отпечатка однм действием count_xor_bits (искать в standard, numpy, scipy)
    - ускорение Numba (наскоком - не получилось) - на count_xor_bits

- проверить предположения по улучшению подбора совпадений:
  - central_correlation вместо корреляции от начала

- список файлов для фингерпринта нужно конечно готовить заранее, однако можно его поделить на какие-то части, по корневым папкам, например, чтобы сообщать: проверяется такая-то папка. Типа, перед продолжением - проверить, нет ли изменений в непроверенной части.

- поиск идей для улучшения фингерпринтов:
  - https://news.ycombinator.com/item?id=8303713
  - https://github.com/topics/audio-fingerprinting?l=python&o=asc&s=stars

- запуск с аргументами:
  - -i --info
  - -s --scan
  - -c --correlate
  - -f --full
  - -p --pair
  - https://habr.com/ru/company/ruvds/blog/440654/
  - https://docs.python.org/3/library/argparse.html


## Отбор из дубликатов аудиодорожки лучшего качества

- Сравнить аудио.
  - Если есть похожий - добавить к нему в группу.
  - Если похожих нет - создать новую группу.
- Показать все группы, в которых  больше одного трека, пользователю. Сортировать по выигрышу места = сумма размеров файлов - размер крупнейшего файла.
  - Внутри каждой группы сортировать по: длина, битрейт.
  - Все тэги внутри группы - объединяются в один набор мета-данных. Пользователь может отменить объединение (чтобы объединить руками) или подтвердить - тогда все файлы получат этот тэг.
  - Пользователь выбирает один трек, который остаётся - Комета удаляет остальные треки, и удаляет группу из памяти, переходит к следующей группе.



# Другие задачи


## Добавление обложки в файл со звуком

1. Выбираем файл (JPEG, PNG, TIFF)
2. Вручную выделяем кроп-область 1:1 или Custom, автообрезаем.
3. Автоуменьшаем под точный размер или на целую величину, чтобы попасть в
промежуток между min и max шириной и высотой.
4. Сохраняем в jpeg в тэге.


## Резервирование коллекции

Периодически:
- подключиться к Yandex Music
- получить шорт-треки из плейлистов
- если есть track_id, которых нет в базе:
	- скачать мету, аудио и обложку
    - объединить их в файл
	- добавить в мету файла:
	  - replay gain
	  - hash of audio stream (чтобы быстро обнаруживать точные совпадения)
	  - fingerprint (отслеживать похоие треки)
	- добавить мету в базу данных (а стрим - можно и в файлы)




# Остальная полезная инфа

#№ Потребление памяти записями

Измерение проводилось по Диспетчеру задач Windows 10. Точность для памяти - 0.1  Гб. Для накопителя - 1 байт. Для улучшения точности нужно будет провести замеры с профайлером памяти, не требующим ресурсов.

Запись одной корреляции занимает:
- JSON по умолчанию - в файле: 323 байта;
- json по умолчанию - в памяти: 550 байт;
- pandas.DataFrame - в памяти: 550 байт;
- ijson - в памяти: 650 байт;
- json пик при загрузке - в памяти: до 840 байт (10 млн записей за 36.6 сек);
- pandas при загрузке в DataFrame из JSON: до 1150 байт (за 27.1 сек);
- ijson при загрузке - 680 байт (за 38 сек).

Итого:
- при загрузке крупных файлов потребление памяти в 3.5 раза больше, если загружать напрямую в pandas, а после загрузки - в 1.7 раза больше, чем загруженный стандартный файл.
  - Если на компьютере установлено "8 ГБ", то при Windows 10 по-умолчанию свободны будут около 3,5 ГБ, и чтобы работать без зависаний,  пригоден файл JSON до 1 ГБ.
- Если нужно забить память под завязку - удобен пакет ijson. Он читает чуть медленнее, и потребляет больше памяти, однако загружает "плавно", без требования памяти с запасом для операций.